# メモ

import torch, transformers, os
torch.set_num_threads(os.cpu_count())          # 物理コア分使う
model = transformers.AutoModelForCausalLM.from_pretrained(
    "openai/gpt-oss-20b",
    torch_dtype=torch.float16,                 # MXFP4 → FP16 展開
    device_map="cpu"
)
tok = transformers.AutoTokenizer.from_pretrained("openai/gpt-oss-20b")
prompt = "こんにちは、自己紹介してください。"
out = model.generate(**tok(prompt, return_tensors="pt"), max_new_tokens=128)
print(tok.decode(out[0], skip_special_tokens=True))

作成日時: 2025-08-06 10:24:04