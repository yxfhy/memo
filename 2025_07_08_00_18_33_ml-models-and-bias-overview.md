# 機械学習モデルとバイアス一覧

ご主人さま、機械学習の主要モデルと代表的なモデルバイアスを一覧で並べます。

線形回帰：入力特徴と出力が線形に結合する／効果は加法的／誤差はガウス分布を仮定

ロジスティック回帰：クラス確率の対数オッズが線形／分離境界は一次平面

SVM（カーネル SVM 含む）：マージン最大化／決定は少数のサポートベクトルに依存／カーネルで高次元線形分離を仮定

決定木・LightGBM：軸に平行な階段関数で分割／局所ルールの集合／特徴の重要度順に逐次分割

Random Forest（バギング系）：複数の決定木を平均化して分散を低減／各木はデータと特徴をランダムサブサンプル

k-NN：近傍点は同じラベル・値を持つと仮定／距離尺度への強い依存／局所平滑性

Naive Bayes：特徴は条件付き独立／単純な確率生成モデル

主成分分析（PCA）：データは線形低次元空間に集中／最大分散方向が情報を保持

線形判別分析（LDA）：クラスごとに同一共分散ガウス／線形境界

CNN：入力は平行移動不変な局所パターン／重み共有と局所受容野／階層的特徴

RNN・LSTM：系列は時間順序に依存／情報は隣接時間帯に強く偏る／長期記憶はゲートで制御

トランスフォーマー：全位置自己注意で長距離依存を直接担保／位置情報は外部エンコーディング／データは完全結合グラフとみなす

グラフニューラルネットワーク（GNN）：ノードは局所近傍だけ相互作用／パーミュテーション不変／エッジ構造が学習を制約

E(3) 同変 GNN：物理 3D 系で回転・平行移動同変性を厳守

Autoencoder：データは低次元潜在空間で再構成可能／自己教師で情報圧縮

生成的拡散モデル：データ生成過程をマルコフノイズ→逆拡散とみなす／滑らかな連続変換

GAN：二者ゲームで分布一致を学習／生成分布は低次元乱数→データ空間写像

ガウス過程回帰：関数空間の事前分布／カーネルで滑らかさを暗黙に仮定

k-means：クラスタは球状・等分散／L2 距離で最近点割当て

DBSCAN：データは高密度領域と低密度領域に分かれる／形状フリー

ベイズ線形回帰：パラメータにガウス事前分布／予測に不確実性を付与


以上でございます。


ご主人さま、機械学習の主要モデルと代表的なモデルバイアスを一覧で並べます。

線形回帰：入力特徴と出力が線形に結合する／効果は加法的／誤差はガウス分布を仮定

ロジスティック回帰：クラス確率の対数オッズが線形／分離境界は一次平面

SVM（カーネル SVM 含む）：マージン最大化／決定は少数のサポートベクトルに依存／カーネルで高次元線形分離を仮定

決定木・LightGBM：軸に平行な階段関数で分割／局所ルールの集合／特徴の重要度順に逐次分割

Random Forest（バギング系）：複数の決定木を平均化して分散を低減／各木はデータと特徴をランダムサブサンプル

k-NN：近傍点は同じラベル・値を持つと仮定／距離尺度への強い依存／局所平滑性

Naive Bayes：特徴は条件付き独立／単純な確率生成モデル

主成分分析（PCA）：データは線形低次元空間に集中／最大分散方向が情報を保持

線形判別分析（LDA）：クラスごとに同一共分散ガウス／線形境界

CNN：入力は平行移動不変な局所パターン／重み共有と局所受容野／階層的特徴

RNN・LSTM：系列は時間順序に依存／情報は隣接時間帯に強く偏る／長期記憶はゲートで制御

トランスフォーマー：全位置自己注意で長距離依存を直接担保／位置情報は外部エンコーディング／データは完全結合グラフとみなす

グラフニューラルネットワーク（GNN）：ノードは局所近傍だけ相互作用／パーミュテーション不変／エッジ構造が学習を制約

E(3) 同変 GNN：物理 3D 系で回転・平行移動同変性を厳守

Autoencoder：データは低次元潜在空間で再構成可能／自己教師で情報圧縮

生成的拡散モデル：データ生成過程をマルコフノイズ→逆拡散とみなす／滑らかな連続変換

GAN：二者ゲームで分布一致を学習／生成分布は低次元乱数→データ空間写像

ガウス過程回帰：関数空間の事前分布／カーネルで滑らかさを暗黙に仮定

k-means：クラスタは球状・等分散／L2 距離で最近点割当て

DBSCAN：データは高密度領域と低密度領域に分かれる／形状フリー

ベイズ線形回帰：パラメータにガウス事前分布／予測に不確実性を付与


以上でございます。



作成日時: 2025-07-08 09:18:29