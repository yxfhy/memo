# メモ

主要なNR-IQAモデル（2019年～2025年）

DBCNN (2019年): 合成歪みと実写歪みの両方に対応するDeep Bilinear CNNモデル。合成歪み用と実写歪み用の2つのCNN枝でそれぞれ特徴を抽出し、ビリニアプーリングで融合して品質スコアを予測する。合成・実写の両データセットにおいて既存手法を上回る性能を示した。

MetaIQA (2020年): ResNet18をバックボーンとする深層メタラーニングモデル。合成歪みデータ（TID2013, KADID-10K）でメタ学習を行い、実写歪みデータ（CID2013, LIVE Challenge, KonIQ-10k）に適用する。メタ学習により未知の歪みにも迅速に適応し、従来手法（BLIINDS-II, BRISQUE, CORNIA, HOSA, WaDIQaM-NRなど）を大幅に上回る相関性能（SRCC, PLCC）を達成した。

HyperIQA (2020年): CNNと自己適応型ハイパーネットワークを組み合わせたモデル。入力画像から内容表現を抽出し、ハイパーネットワークで歪み判別ルールを動的に生成して品質予測器に適用する。実写歪みデータ（LIVE Challenge, KonIQ-10kなど）上で従来の最先端法を凌駕する性能を示し、合成歪みデータセットでも競合的な結果を得た。

TranSLA (2021年): Vision Transformerベースのモデルで、サリエンシーマップ（注目領域）と画像勾配マップによる局所情報を組み合わせる。Saliency-Guided機構で評価領域を強調し、Gradientマップ埋め込みで微細構造を補完、さらにBoosting Interaction Moduleでトークン間の相互作用を強化する。KonIQ-10kやCLIVEなどの大規模な実写歪みベンチマーク上で従来手法を大幅に上回る性能を示した。

TReS (2022年): CNNとTransformerを組み合わせたハイブリッドモデルで、相対ランク学習（relative ranking loss）や入力画像の反転による自己一貫性損失を導入し、局所・非局所特徴を同時に学習する。合成歪み（LIVE, CSIQ, TID2013, KADID-10K）と実写歪み（CLIVE, KonIQ-10k, LIVEFB）の両方で評価し、特に大規模データセット（LIVEFB, KADID-10K）で従来手法を大幅に上回り、全体の加重平均性能でも最良結果を達成した。

LoDa (2023年): 大規模事前学習済みVision Transformerに対し、CNNから抽出したマルチスケールの局所歪み特徴をクロスアテンションで注入する効率的適応手法。LoRAによる軽量アダプタで少ないパラメータ増加で学習可能とする。複数のベンチマーク（LIVE, KonIQ-10k, KADID-10K, SPAQ, FLIVE）におけるクロスデータセット評価で全ての条件下で最良のSRCCを達成し、高い汎化能力を示した。

SaTQA (2024年): 教師ありコントラスト学習（Supervised Contrastive Learning）とTransformerを組み合わせたモデル。まず大量の合成歪み画像でコントラスト学習により歪み特徴を学習し、その後Multi-Stream Block（CNNの局所バイアス＋Transformerの長距離依存性）とPatch Attention Blockで特徴を融合して品質スコアを予測する。標準的な合成・実写歪みデータセット7種類で評価し、いずれも従来最先端法を上回る性能を示した。

MDIQA (2025年): “Multi-Dimensional IQA”の枠組みを提案するモデル。画像品質を5つの技術的次元（シャープネス、ノイズ量等）と4つの美的次元（構図、雰囲気等）で捉え、各次元に対して別個のヘッドを持つマルチブランチネットワークを構築する。各次元の予測値は画像ごとに動的に重み付けされて総合スコアに統合される。CLIVE, KonIQ-10k, SPAQ, FLIVEなど複数のデータセットで学習・評価を行い、これらのベンチマーク上で従来手法を上回る最先端性能を達成した。また、既存手法（WaDIQaM, DBCNN, HyperIQA, TReS, LoDaなど）との比較でも、クロスデータセット評価で一貫して大きく優位であることが示されている。


参考文献: 各モデルの詳細は引用文献を参照のこと。



作成日時: 2025-09-25 09:15:20