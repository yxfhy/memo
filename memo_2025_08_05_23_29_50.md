# メモ

# load_model.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL = "openai/gpt-oss-7b"     # 仮の 7B 名称。実際のリポ名に置換

model = AutoModelForCausalLM.from_pretrained(
    MODEL,
    load_in_4bit=True,          # 4bit 量子化
    torch_dtype=torch.float16,  # 計算は FP16
    device_map="auto",          # 自動割当
    max_memory={0: "10GiB", "cpu": "30GiB"}  # GPU に余裕を残す
)

tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=False)

作成日時: 2025-08-06 08:29:48