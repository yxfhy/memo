# メモ

深層学習による主観的動画品質評価の進展 (2020年以降)

はじめに

動画品質評価（Video Quality Assessment, VQA）は、動画の画質を人間の主観評価に近い形で定量化する技術です。近年、ディープラーニングの発展と大規模主観評価データセットの整備により、従来の手法を凌ぐ高精度なVQAモデルが数多く提案されています。本稿では、2020年以降に発表された深層学習ベースの主観的動画品質評価に関する代表的な研究論文5件と、それを支えた重要なデータセット5件を紹介します。それぞれの論文・データセットが果たした貢献と、主観品質評価分野の進展への影響について解説します。

主要な研究論文5選

Patch-VQ（CVPR 2021） – 大規模データセットLSVQとパッチベースVQAの提案

Patch-VQ: 'Patching Up' the Video Quality Problemは、Bovikらのグループによる研究で、当時最大規模の主観評価データセットLIVE-FB LSVQ（後述）を構築し、局所パッチ情報を活用する新しいVQAモデルを提案しました。彼らはまず38,811本の実動画に対し約550万件の主観スコアを収集し、既存より「桁違いに大規模」で「多様性に富む」データセットを公開しました。このデータセット上で局所から全体へのリージョンベースの深層学習モデル(PVQ)を学習し、複数のUGC動画データセットで当時の最先端精度を達成しています。Patch-VQの貢献は、大規模データによる学習で主観品質予測の精度を大きく向上させた点と、空間的・時間的な劣化を局所パッチ単位で捉える手法を提示した点にあります。このアプローチは以後の研究で広く参照され、動画内の品質劣化部分を局所的にマッピングする手法（品質マップ）の先鞭ともなりました。

UGC-VQA/VIDEVAL（TIP 2021） – ユーザー生成動画向けブラインドVQAのベンチマーク

UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Contentは、Texas大学LIVE研究所とGoogle/YouTubeチームの共同研究で、雑多なユーザー生成コンテンツ(UGC)動画の品質評価に焦点を当てています。著者らはコーデック歪みだけでなく撮影時の様々な劣化を含むUGC動画に対し、多数の最新ノンリファレンスVQA手法を評価する統一フレームワークを構築しました。既存の763種類の特徴量を比較検討し、その中から有効な60種の特徴を選抜・融合した軽量モデルVIDEVALを開発しています。VIDEVALは他手法と遜色ない精度でありながら計算効率に優れ、当時の最先端性能を達成しました。この研究の意義は、大規模UGCデータ上で各種手法の包括的ベンチマークを行い、最適な特徴融合による高精度・高効率モデルの設計指針を示した点にあります。また、この研究により策定されたベンチマークは、その後の深層学習VQA開発・評価の標準枠組みとなり、本分野の研究を加速させました。

RIRNet（ACM MM 2020） – 再帰構造を活用した時空間特性モデリング

RIRNet: Recurrent-In-Recurrent Network for Video Quality Assessmentは、Shiらによる2020年の研究で、動画の時間的情報を効果的に捉えるため入れ子構造の再帰型ネットワークを導入しました。RIRNetは大きく2つの部分から構成されます。まず品質劣化学習サブネットとしてResNet-50ベースのCNNで各フレームから歪み感知特徴を抽出し、次に動き効果モデリングとして階層型のRNNネットワークでフレーム間の時系列変化を捉えます。この二段階の再帰構造により、映像内の静的な画質低下要因と動的な揺らぎ（ブレやフレームレート低下の影響など）を両面から評価できるようになりました。RIRNetは当時の他手法と比較しても良好な精度を示し、特に時間的な品質変動への対応力で注目されました。本研究は、動画品質評価における時系列情報の重要性を再認識させ、以降のLSTMやTransformerを用いた時空間統合モデルの発展につながりました。

FAST-VQA（ECCV 2022） – フラグメントサンプリングによる高効率エンドツーエンドVQA

FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Samplingは、Wuらが提案した高速・高精度なVQAモデルです。高解像度動画の評価では計算コストが膨大になる問題に対し、本研究ではGrid Mini-patch Sampling (GMS)という新手法を導入しました。これは映像フレームを格子状に分割し、各グリッドから高解像度パッチを抜き出すことで局所画質情報を保持しつつ、間を低解像度ミニパッチで埋めて大局的文脈もカバーするものです。これらのミニパッチ列（フラグメント）を入力とする専用ネットワークFragment Attention Network (FANet)を設計し、画質関連特徴を効率良く学習できるエンドツーエンドモデルを実現しました。FAST-VQAは従来比約10%の精度向上と99.5%のFLOPs削減を達成し、解像度の異なる動画にも高い汎用性を示しています。この成果は、リサイズやクロップといった従来の単純サンプリングでは画質情報を損なう問題に対する革新的解決策であり、大規模・高解像度動画の実用的な品質モニタリングを可能にしました。

DOVER（ICCV 2023） – 審美と技術の二視点からの主観品質評価モデル

DOVER: Disentangled Objective Video Quality Evaluatorは、Wuらの2023年の研究で、動画の主観品質に影響を与える要因を「美的品質」と「技術的品質」に分離して評価する新しい枠組みを提案しました。著者らはまず3,590本のUGC動画について、映像の内容魅力（美的観点）と歪み劣化（技術観点）それぞれに対する主観評価を45万件収集し、世界初の分離評価付きデータセットDIVIDE-3kを構築しました。分析により、UGC動画の品質判断にはこの両面が広く影響していることを示し、そこでDOVERモデルでは2本の評価ブランチを用意し、一方は美的要素（内容の魅力や構図など）、もう一方は技術要素（ノイズやブロックノイズ等の歪み）に特化して特徴抽出を行います。両ブランチの出力を主観スコアに合わせて融合することで、総合品質を予測するとともに、美的・技術それぞれの品質スコアも算出可能です。DOVERは既存のUGC-VQAデータセット上で最先端の精度を実現しつつ、各観点での評価内訳を提供できるため、従来ブラックボックスだったモデルに説明性と実用的指標をもたらしました。この研究は、主観品質評価における知覚要因の解釈性という新たな方向性を開拓したと言えます。

重要なデータセット5選

YouTube-UGCデータセット (2019/2020) – 大規模UGC動画の主観評価コーパス

YouTube-UGC Datasetは、YouTubeのメディアアルゴリズムチームが公開したユーザー生成コンテンツ動画の大型データセットです。約1,500本の20秒クリップからなり、解像度も360p～4Kまで多岐にわたる多様なカテゴリの映像が含まれています。各動画には100人以上の評価者による主観評価（MOS）が付与されており、その値は1(低品質)～5(高品質)の範囲です。加えて、各動画の前半・中盤・後半（重複する3区間）についてもMOSが提供され、シーン変化による品質影響の分析が可能です。本データセットは2019年に学術ワークショップで紹介され、以降UGC動画の画質評価研究における標準ベンチマークとして広く利用されています。実撮影に由来するブロックノイズ・ブレ・ピンぼけ・露出不良など多様な劣化要因を網羅しており、ディープラーニングモデルの訓練に必要な現実歪みデータを大量にもたらしました。

KonVid-150kデータベース (IEEE Access 2021) – 粗・精二段構成の15万動画データセット

KonVid-150kは、独コンスタンツ大学グループが公開した大規模インザワイルドVQAデータセットです。その名の通り約15万本という極めて大量の動画を収録し、ディープラーニングによる大規模学習に適したリソースとなっています。本データセットは特徴的な二部構成で、まずK150k-A部分では152,265本 (5秒長)の動画それぞれに5件ずつという「粗い」主観評価を付与しています。これは膨大な動画に低コストでラベルを付け、モデルの事前学習等に活用する狙いがあります。一方K150k-B部分では厳選した1,577本に対し各89件以上（平均約120件）の評価を集め、高精度なMOSを算出しています。この組み合わせにより、全体としては大規模かつ多様なデータを提供しつつ、一部で精度の高い評価を保証するバランスを実現しました。KonVid-150kの登場により、粗アノテーションを含む巨大データからの効率的学習や、予算配分戦略の異なるVQA手法の検証が可能となり、効率性と性能を両立する新手法（例: FAST-VQAなど）の研究を後押ししました。

LIVE-FB LSVQデータセット (CVPR 2021) – 史上最大規模の主観評価ビデオコーパス

LSVQ (LIVE-FB Large-Scale Social Video Quality) データセットは、Texas大学LIVE研究所とFacebook AIの共同で構築された超大規模UGC動画品質データセットです。38,811本の多様な動画クリップ（長さは数秒～数十秒）に対し、クラウドソーシングで延べ約6,300人から5,500,000件以上の主観評価を収集しています。各動画からは空間的・時間的局所片（v-patch）が抽出され、それぞれに局所品質スコアも付与されている点が特徴です。動画はInternet ArchiveやYFCC100M等の大規模集合から無作為抽出されており、内容・解像度・歪みタイプのバランスに配慮した構成となっています。LSVQは「現実環境下の天然劣化」を網羅したサイズ・内容両面で世界最大のVQAデータセットであり、このデータ上で学習することで深層モデルの精度と頑健性が飛躍的に向上しました。実際、本データセットを用いたPatch-VQ研究では当時のSOTA精度が達成されており、以降もFAST-VQAやDOVERなど多くの最新モデルが事前学習にLSVQを活用しています。本データセットは主観品質評価分野にスケールメリットをもたらし、ディープラーニングによる汎用的な品質予測モデルの土台を築きました。

UHD-VQ5kデータセット (Neurocomputing 2024) – 4K解像度動画の専門評価データセット

UHD-VQ5kは、近年登場した超高精細(UHD)動画専用の品質評価データセットです。解像度3840×2160（4K）、長さ10秒、30fpsという統一仕様の動画5,500本から構成されます。既存のKonViD-1kやLIVE-Qualcommなど従来データセットはHD程度までの解像度が中心であり、4K固有の問題（高周波ノイズ、圧縮劣化の見え方など）への対応が課題でした。そこで本データセットでは各動画に対しITU-R BT.500勧告に準拠した厳密な主観評価を専門家ベースで実施し、高信頼なMOSを提供しています。また、データセット公開論文ではResNetとVision Transformerを組み合わせたハイブリッドモデル（HR-VQA）を提案し、UHD-VQ5k上でSOTA精度を達成しています。UHD-VQ5kの意義は、従来不足していた4K動画の品質評価基盤を研究コミュニティに提供したことです。これにより高解像度映像ストリーミングや8K時代を見据えた次世代VQAモデルの検証が可能となり、実応用に直結する指標開発が一層促進されています。

DIVIDE-3kデータセット (ICCV 2023) – 美的・技術二側面の意見を含む世界初の動画品質データセット

DIVIDE-3k (Disentangled Video Quality Database)は、DOVER論文で導入された新概念の主観評価データセットです。対象は3,590本のUGC動画で、各動画について従来型の総合品質スコアだけでなく、「映像内容の魅力（美的観点）に対する評価」と「映像の技術的品質（歪みや画質面）に対する評価」の二種類の主観スコアが与えられています。これらは全て実験室環境で精密に測定され、総計45万件に及ぶ評価意見が含まれます。DIVIDE-3kのユニークな点は、人間の品質判断を構成する要因をデータとして分離したことです。分析によれば、UGC動画の主観品質はコンテンツの面白さ・美しさと技術的画質の双方に大きく影響されることが確認されました。本データセットにより、モデルがどの要因にどれだけ反応してスコアを出しているかを検証することが可能となり、VQAモデルの透明性・説明性評価という新たな研究方向が開拓されました。DIVIDE-3kはまだ登場したばかりですが、その概念は品質評価のみならず映像美学評価やパーソナライズ品質予測への応用も期待されています。

主要論文・データセット一覧

1. Patch-VQ (CVPR 2021) – 世界最大規模の動画品質データセットLSVQを構築し、パッチ単位の局所画質特徴を学習するモデルを提案。UGC動画のブラインド品質評価で当時のSOTAを達成。


2. UGC-VQA / VIDEVAL (TIP 2021) – UGC動画向けBVQA手法の包括的評価ベンチマークを構築し、既存特徴を融合した軽量モデルVIDEVALを開発。高精度・低計算コストを両立する新基準を確立。


3. RIRNet (ACM MM 2020) – 再帰型ネットワーク内にさらにRNNを組み込む構造で時間的劣化を効果的にモデリング。空間劣化と動的劣化を同時に学習し、NR-VQAの精度向上に貢献。


4. FAST-VQA (ECCV 2022) – Grid Mini-patch Samplingにより高解像度動画の画質情報を損なわず抽出し、Fragment注意機構で効率良く評価。従来比大幅な高速化と精度向上を達成。


5. DOVER (ICCV 2023) – 美的品質と技術的品質の二視点に画質要因を分離して学習する初のVQAモデル。対応するDIVIDE-3kデータで主観メカニズムを解析し、SOTA精度と高い説明性を実現。




---

1. YouTube-UGC Dataset (MMSP 2019/ICIP 2020) – 約1,500本の20秒UGC動画から成る大規模データセット。解像度360p～4K、各動画100人以上の評価によるMOS付与。UGC画質研究の標準ベンチマーク。


2. KonVid-150k (IEEE Access 2021) – 152,000本以上の5秒動画に粗評価5件ずつ＋約1,600本に精密評価89件以上という二段構成データセット。大規模コーパス上での効率的学習と検証を可能にした。


3. LIVE-FB LSVQ (CVPR 2021) – 38,811本・約550万評価という史上最大の主観動画品質データセット。多様な実動画と膨大なラベルによりディープVQAモデルの性能と汎化性を飛躍的に向上させた。


4. UHD-VQ5k (Neurocomputing 2024) – 4K解像度・10秒・30fpsの動画5,500本で構成されるUHD画質評価データセット。専門家評価に基づく高信頼MOS付きで、次世代高解像度VQA研究を促進。


5. DIVIDE-3k (ICCV 2023) – 3,590本のUGC動画に対し、美的観点と技術観点の二種類の主観スコアを収録した初のデータセット。品質評価モデルの解釈性・要因分析に新たな道を開いた。




作成日時: 2025-10-27 16:18:21