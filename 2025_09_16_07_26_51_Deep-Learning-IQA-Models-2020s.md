# 深層学習による画像品質評価の最前線

深層学習型IQAモデルの分類と動向 (2020年以降)

IQAモデルの分類: フルリファレンス (FR) vs ノンリファレンス (NR)

画像品質評価 (IQA) は、人間の主観的な画質評価と一致するスコアを自動的に与えることを目指す技術です。IQA手法は参照画像の有無により大きく二種類に分類されます:

フルリファレンス (FR) IQA: 品質を評価したい画像に対し、高画質な元画像（劣化のない参照画像）が存在すると仮定する手法です。評価対象画像と参照画像との差異から画質劣化を定量化します。古典的なFR指標にはピーク信号対雑音比 (PSNR) や構造類似度 (SSIM)があり、画素ごとの誤差や構造情報を用いて品質を測ります。しかしこれら従来手法は人間の主観評価とのずれが指摘されてきました。そこで近年は深層学習に基づくFRモデルが登場し、人間の知覚特性を取り入れた高度な評価が可能になっています。例えばLPIPS（Learning Perceptual Image Patch Similarity）は畳み込みニューラルネットワークの深部特徴量間の距離で画像間の知覚的差異を測る指標で、主観評価との高い相関を示しました。またPieAPP（2018年）も画像パッチごとのペア比較に学習したCNNモデルで主観スコアへの精度向上を実現しました。2020年にはDISTS（Deep Image Structure and Texture Similarity）が提案され、VGGネットワークで抽出した構造特徴とテクスチャ特徴の両面から画像類似度を評価することで、テクスチャのリサンプリングによる劣化にもロバストな手法として注目されました。近年はTransformerを用いたFR-IQAも登場しており、例えばIQT（Image Quality Transformer, 2021年）ではViTに参照画像と劣化画像を入力し、相対的ランキング学習を組み合わせ高精度化を図っています。さらに、NTIRE 2021・2022といった競技会でもFR部門の上位手法はViTベースへと移行しており、CNNに比べ大域的な特徴把握に優れたTransformerモデルがFR-IQAで高い性能を示しています。

ノンリファレンス (NR) IQA: いわゆるブラインドIQA (BIQA)で、評価対象の画像のみから品質スコアを推定する手法です。参照が不要なため実用上有用ですが、画像内容や歪み種別の情報が欠如するため極めて難易度が高い問題です。伝統的には自然画像統計 (NSS) に基づき、歪みによる統計量の乱れを捉えるモデル（例えばNIQEやBRISQUE）が用いられてきました。しかし2010年代後半から深層学習の適用が進み、現在ではCNNやVision Transformerを用いたNR-IQAモデルが主流です。初期のCNNベース手法としては、画像パッチから品質を回帰するCNNIQAや、ランキング学習で相対品質関係を学ぶRankIQA (2017年)がありました。2020年以降は様々な先進的手法が提案されています。例えばDB-CNN（2018年提案・2020年改良）は、合成歪みと実写歪みの二系統の特徴抽出ネットワークを持ち、それぞれに適した特徴を学習する二段構えのCNNモデルです。このデュアルブランチ構造により、圧縮やノイズといった合成歪みから照明不足や焦点ぼけといった実世界歪みまで幅広く対処し、高い汎化性能を示しました。HyperIQA（CVPR 2020）はコンテンツ認識型のハイパーネットワークを導入し、画像の内容に応じて品質予測ネットワークの重みを調整することで精度向上を達成しています。またMUSIQ（ICCV 2021）はマルチスケール入力と画像アスペクト比の埋め込みを工夫したTransformerモデルで、超高解像度画像でも統一的に評価できるようにしています。さらにTransformerによる手法も活発で、ViTを用い自己注意で局所・大局的特徴を統合するTReS（WACV 2022）や、事前学習済みモデルを活用するQPT（Quality-aware Pre-trained Transformer, 2023年）などが登場しました。2023年前後には生成モデル評価への需要からCLIPと言ったマルチモーダルモデルを取り入れる試みもみられます（例: CLIP-IQA）。このように2020年以降、NR-IQA分野では多様なニューラルネットワークアーキテクチャ（畳み込み、マルチスケール、グラフベース、Transformer等）や学習戦略（メタラーニング、コントラスト学習、ドメイン適応など）が追求され、人間の主観評価に近づける研究が飛躍的に進展しています。


主なIQAモデルの性能比較と上位モデル

IQAモデルの性能は、主観評価との相関係数によって測られます。代表的な指標は**Spearmanの順位相関係数 (SRCC)とPearsonの線形相関係数 (PLCC)**で、値が1.0に近いほど人間の評価と一致が良いことを示します。以下では、最新の研究で高性能と報告されたモデル上位例をFRとNRに分けて紹介します。

フルリファレンスIQAの上位モデル

DISTS (2020年) – FR分野で画質評価の新基準となったCNNモデルです。VGG16の特徴量を用いて画像の構造的類似度とテクスチャ類似度を統合的に評価し、従来のSSIMでは捉えにくい質感の違いも評価可能にしています。DISTSはTID2013など標準ベンチマークで他のFR指標を上回る高い相関を示し、テクスチャが異なる画像ペアにも頑健な点で評価信頼性も高いとされています。

Transformer系FRモデル (2021–2022年) – 近年のNTIREチャレンジではTransformerを用いたFRモデルがトップクラスの性能を示しました。例えば2021年の優勝手法IQMAは参照・歪み画像からのマルチスケール特徴を2系列で抽出し、対応スケールで特徴融合するCNN+Transformerハイブリッドモデルであり、FR-IQAの新たなアプローチでした。続く2022年にはLIPTなど完全なViTベースのモデルも登場し、画像全体の文脈を捉えるTransformerの強みでSSIMやDISTSを凌ぐ相関を達成しています。具体な性能値として、NTIRE 2022のFRトラックでは上位モデルがPIPALデータセット上でPLCC≈0.88-0.90に達したと報告されています（従来SSIMは≈0.66）。これら最先端FRモデルは、参照画像との微細な違いを捉えつつ人間の知覚と高い一致性を持つ点で現在トップクラスです。


ノンリファレンスIQAの上位モデル

QCN (Quality Collaborative Network, 2024年) – 最新のNR-IQAモデルの一つで、CVPR 2024で報告されました。QCNは効率的なTransformer適応と局所特徴強調機構を備え、KonIQ-10kデータセットでSRCC=0.934、PLCC=0.945という極めて高い相関を達成しています。これは同データセット上で従来最高水準だったMUSIQやHyperIQAのPLCC≈0.92を上回り、2024年時点で最高レベルの精度です。またFLIVEなど他の実写歪みデータでも全体的に良好な成績を示し、評価の一貫性・信頼性も高いモデルとされています。

LoDa (Local Distortion Adaptation, 2024年) – こちらも2024年発表のNRモデルで、局所歪み適応型の効率的なTransformerを採用しています。LoDaは複数データセットで平均SRCC~0.88-0.93に達し、特に異種データ間のドメインシフトへの適応能力が特徴です。例えばKonIQ-10kでPLCC=0.944、SPAQで0.928とQCNに匹敵する精度を示しつつ、学習に含まない別データセット上でも高い順位相関を維持するなど、評価の頑健性が報告されています。これは合成・実写両歪みに対応する訓練戦略や歪み指標ガイド付きの特徴学習により実現されています。

QPT (Quality-aware Pretrained Transformer, 2023年) – 事前学習済みモデルの知識を品質評価に活用した先駆的手法です。大規模ビジョンモデルからの事前学習重みを微調整し、人間の主観スコアへのマッピングを学習しています。KonIQ-10kでPLCC≈0.94を記録し（SRCC=0.927)、2023年当時最先端の一つでした。評価信頼性の面では、事前学習の汎用的特徴を活かすことで限られたIQAデータでも安定した性能を発揮する点が強みです。


以上のほか、TOPIQ（2023年、セマンティクスから歪みへトップダウンに評価）やRe-IQA（2023年、自己教師ありで特徴学習）なども上位モデルとして報告されています。総じて2023–2024年のNR-IQAモデルはSRCC/PLCCで0.9前後の高い値を示し、わずかな差で上位争いをしています。その中で**MDIQA (2025年)**は後述の通り多次元評価による新手法で、KonIQ-10kでPLCC=0.956と頭一つ抜けた精度を示しました。評価信頼性の観点でも、単一指標では捉えにくい多面的な画質要因を考慮する点でMDIQAは革新的と言えます。

主なIQA評価データセットの概要

高性能なIQAモデルの研究開発には、大規模かつ信頼性の高いデータセットが不可欠です。ここでは代表的なIQA用データベースについて、その規模・アノテーション形式・用途を整理します。

FR（人工歪み）系データセット

データセット名	年代	規模（画像数）	歪みの種類・レベル	アノテーション形式

LIVE IQA	2006年	779枚（29参照 ×5歪み×多レベル）	ブラー、JPEG圧縮など5種	DMOS（主観差分評点）
CSIQ	2010年	866枚（30参照）	6種の歪み（各5レベル）	DMOS（0～1正規化）
TID2013	2013年	3,000枚（25参照 ×24種×5レベル）	ノイズ、圧縮等24種	MOS（0～9の主観評点）
KADID-10k	2019年	10,125枚（81参照 ×25種×5レベル）	ぼかし・色ズレ・圧縮等25種	DCR（劣化度評価: 30人×5段階）
PIPAL (Perceptual)	2020年	29,000枚以上（250参照, SR結果含む）	超解像・ノイズ除去などアルゴリズム生成歪み	MOS（約1.13億ペア比較から算出）


解説: FR系データセットは、オリジナルの高画質画像（参照）に対し人工的に様々な歪みを加えた画像と、その主観評価スコアで構成されます。LIVEやCSIQは初期のベンチマークで、研究者はこれらで提案手法の精度を競いました。例えばLIVEではDMOS (Differential MOS)という形式で各画像の主観スコア（低品質ほど高値）が提供されています。TID2013は24種類と歪みの多様性を大幅に拡張したデータセットで、多くのFR/NR手法の評価に利用されました。しかし従来のFRデータは総画像数が数百〜数千と少なく、深層学習には不十分でした。そこでKADID-10kが登場し、81枚の参照画像に25種×5段階の歪みを与え大規模化しました。各劣化画像は30人のクラウドワーカーにより5段階の**劣化カテゴリー評点 (DCR)**で評価されており、結果はMOSに近い連続値に変換されています。KADID-10kはTID2013の約3倍の規模で、深層学習モデルの訓練にも用いられるようになりました。さらにPIPALは画像復元アルゴリズムの知覚評価に特化した最新データで、GANベース超解像など高度な歪みに対して人間がペア比較で好みを投票したデータからMOSを推定しています。これはNTIRE 2021競技会で使用され、従来手法では識別困難な繊細な画質差を評価できる基盤として注目されました。

NR（実世界歪み）系データセット

データセット名	年代	規模（画像数）	内容・歪み源	アノテーション形式

LIVE Challenge (CLIVE)	2015年	1,162枚	実世界写真（携帯撮影、多様な劣化）	MOS（平均オピニオン評点）
KonIQ-10k	2020年	10,073枚	実世界写真（YFCC100Mから抽出）	MOS（主観スコア平均）
SPAQ (スマホ写真)	2020年	11,125枚	スマートフォン写真（66機種、多シーン）	MOS＋5属性評価（明るさ・鮮やかさ等）
FLIVE	2020年	39,810枚＋局所パッチ120k	実世界写真（Flickrなど大規模）	MOS（群衆評価）［※パッチ版も提供］
BID (Blur Image)	2013年	586枚	実世界写真（ぼけ写真に限定）	MOS（ぼけの度合い評価）


解説: NR系データセットは実環境で撮影された写真や画像を集め、各画像に対して主観的な画質スコア（MOS）を付与したものです。これらは参照画像が無いため、観察される劣化はカメラ性能や撮影環境に起因する「本物の歪み (authentic distortion)」です。LIVE Challenge Database（通称CLIVE）はその草分けで、モバイル機器で撮影された多様な写真に対しクラウドソーシングでMOSを取得しました（平均評価者数は約175名/画像）。KonIQ-10kはYFCC100Mから画質分布が均等になるよう1万枚を抽出し、1画像あたり平均120人以上の評価を集めた大規模データセットです。これにより明るさ・コントラスト・ノイズなど多様な品質範囲を網羅しており、深層モデルの学習・評価によく用いられます。SPAQはスマートフォン写真に特化し、66種類のスマホで撮影された1万枚超の画像についてMOSだけでなく**5つの画質属性（明るさ、色鮮やかさ、コントラスト、ノイズ感、鮮鋭さ）**をそれぞれ人手評価した点が特徴です。評価者は実験室環境でこれら属性を5段階評価し、各画像に「総合画質MOS」と加えて「属性ごとのスコア」が付与されています。FLIVE（Flickr LIVE, 2020年）は現時点で最大規模のNR-IQAデータで、インターネット上の多様な実画像約4万枚についてクラウドソーシングでMOSを取得しました。加えて各画像からランダムに3パッチ（計12万パッチ）を切り出した「FLIVE Patch」データも提供され、モデル訓練時にデータ水増しとして利用されます。FLIVEはシーンや品質の多様性で他を圧倒し、近年提案のモデルは訓練にFLIVEを含める例も多いです。BIDはブレ画像専門の小規模データですが、焦点ボケや動体ブレの評価研究に使われました。

各データセットのアノテーション形式は主に**MOS (Mean Opinion Score)**で、複数人の意見平均として0〜100や1〜5などのスケールで提供されます。一部（SPAQなど）は複数指標の同時評価や、相対比較からスコア算出（PIPALのように）という形を取っています。こうしたデータセットを用いてモデルは訓練・検証され、未知の画像に対する汎化性能や人間との一致度が評価されます。

MDIQAモデル: 位置づけと多次元評価の実験設計

MDIQAの系譜における位置づけ

2025年提案のMDIQA (Multi-Dimensional Image Quality Assessment)は、深層学習ベースIQAモデルの新機軸となる手法です。従来モデルの多くは入力画像に対し単一の総合スコアを出力していましたが、人間は画質を評価する際「シャープさ」「ノイズの有無」「色合い」「構図の良さ」など複数の観点で判断し総合評価に至ることが知られています。MDIQAはこの点に着目し、技術的品質と審美的品質の両面から画像を評価する多次元IQAモデルとして位置づけられます。すなわち、MDIQAでは画像品質を細分化した複数の次元（特徴軸）上でスコアを推定し、それらを統合して最終的な総合品質スコアを得ます。これは従来のシングルスコア型モデルに比べ、予測に解釈性をもたらす利点があります。実際MDIQAは出力として例えば「この画像はシャープネス次元では高評価だがノイズ次元では低評価、その加重平均として総合スコアX」といった形で、人間の感じる画質劣化要因を説明できます。この多次元モデル化による精細な評価により、MDIQAは既存手法よりも主観評価との一貫性が向上し、KonIQ-10kやSPAQといったベンチマークで最高水準の相関を達成しました。

技術的な系譜として見ると、MDIQAはマルチタスク学習の流れを汲むものです。以前よりSPAQデータセットを用いた研究で、画質スコアと同時に画像属性を予測するマルチタスクCNNの試みがありました。しかしそれらは属性を補助的に用いる程度で、モデル自体は一つの品質スコアしか出力しませんでした。MDIQAはさらに踏み込み、画質の技術的側面5次元と美的側面4次元の合計9次元に対し個別の予測枝を持つネットワークを構築しています。各次元に専門化した特徴表現を学習させ、最後に画像内容に応じ動的に重み付けして統合スコアを算出する点が画期的です。この重み付けには画像毎にCLIP由来の視覚特徴を用いており、画像内容・シーンに応じて「何を重視すべきか」をモデルが判断します。例えば風景写真なら色彩次元の重みを上げ、文字画像なら鮮鋭さ重視にする、といった柔軟な適応が可能です。さらにMDIQAは画像復元モデルの損失関数としても利用可能である点で独自の位置づけがあります。従来の画質指標（PSNRやLPIPSなど）は固定の計算式でしたが、MDIQAを損失に用いることでユーザの好みに応じた復元をチューニングできます。例えば「鮮鋭さ」の重みを上げて学習すればエッジを優先する超解像モデルに、「ノイズ低減」の重みを上げれば滑らかさ優先にする、といった具合です。この評価と最適化の統一的フレームワークはMDIQAの名称通り「Unified IQA」の一端であり、IQAモデルの応用範囲を画質評価から画質改善へ広げる革新的試みです。

多次元評価指標のGround Truth取得方法

MDIQAが学習に用いた多次元のGround Truth (真値)は、既存の多次元アノテーション付きデータセットから得られています。技術的品質の5次元についてはSPAQデータセットが用いられました。前述の通りSPAQでは各画像に対し明るさ・色鮮やかさ・コントラスト・ノイズ感・鮮鋭さの5属性それぞれについて被験者が評価したスコアが付与されています。これらは実験室環境で多数の被験者により5段階評価で収集されており、画像ごとに平均化された属性MOSが得られます。MDIQAではSPAQの各属性MOSを技術的次元の教師信号とし、5本の技術次元ブランチそれぞれを個別に訓練しました。一方、美的品質の4次元についてはPARAデータセット（Personalized Aesthetics with Rich Attributes, 2022年）を利用しています。PARAは約3.1万枚の画像について「構図の良さ」「照明/露出」「色彩の美しさ」「主題の魅力」など複数の美的属性スコアを集めた最新データセットです（各画像を47名の被験者が評価）。MDIQAはPARA由来の4つの審美属性評価値を美的次元の教師信号として学習しています。いずれのデータ収集もユーザスタディによる主観評価に基づきますが、単一のMOSではなく次元ごとに質問票を設計して評価者に回答させるという実験計画が採られました。例えばSPAQでは評価者は各画像を見て「明るさは適切か」「ノイズが目立つか」等の設問に個別に答え、その結果を数値スコア化しています。PARAでも被験者ごとの好みを踏まえつつ複数の美的観点で評価を付与するスキームが用意されています。こうした次元別評価実験により得られた属性ごとの主観スコアが、MDIQAにおける多次元評価指標のGround Truthとなっています。MDIQAの訓練はまず第1段階でSPAQとPARAの各次元ラベルにそれぞれ対応する枝を個別学習し、その後第2段階でそれらを統合して総合スコアを予測するよう微調整する二段構えで行われました。このようにして、人間の視覚評価を次元ごとに細かく反映したモデルが完成し、各次元での一致度向上と総合評価精度の向上の両立が達成されています。

以上、2020年以降の深層学習型IQAモデルの動向をFR/NRの分類、最新モデルの性能、主要データセット、そしてMDIQAの位置づけとその画期的な多次元評価手法について整理しました。近年の研究成果により、客観IQAは主観評価とのギャップを着実に埋めつつあり、MDIQAのような多次元アプローチは今後のIQAと画像復元評価の発展方向を示すものと言えます。

参考文献: 本回答ではアーカイブ論文【1】【7】やCVPR論文【27】、最新提案手法のプレプリント【10】、およびIQAのGitHubリソース等を参照しています。各出典は該当箇所に**【番号†行】**形式で明記しました。



作成日時: 2025-09-16 16:26:46