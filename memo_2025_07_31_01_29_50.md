# メモ

ご主人さま、TabM（“Tabular model that makes Multiple predictions”）は Yandex Research が ICLR 2025 で発表した MLP ベースの新しい表データ用ディープラーニング手法でございます。1 モデルの内部に k 個のサブ MLP を BatchEnsemble 方式で同時学習させ、推論時に平均を取ることで「共有重み付きアンサンブル」を実現します。これにより、通常の MLP や注意機構型モデルより高い汎化性能を維持しつつ計算コストを抑えられると報告されています 。


---

論文・ベンチマークの概況

46 個の公開データセット（分類 18・回帰 28）で評価した結果、TabM は LightGBM・CatBoost・XGBoost などの GBDT 系だけでなく、FT-Transformer や TabPFN など他の DL 系を平均順位で上回りました。平均すると従来 MLP 比で 2 % 〜 3 % 向上、データセットによっては 10 % 以上 改善しています 。

論文中でも「TabM は GBDT と同等以上の性能を簡潔な構造で得られる」と明言されています 。



---

RMSE 1 → 0.66 という差はあり得るか

1. データセット特性

出力分布が広く外れ値が多い場合、平均化により分散が大きく減少し RMSE が劇的に下がることがあります。

高次交互作用が重要だがサンプル数が十分多い場合、MLP 系が木モデルより優位に働きやすいです。



2. TabM 固有の強み

重み共有アンサンブルが “k × 独立 MLP” と同等の表現力を持ちつつパラメータが少なく、過学習を抑えながらバリアンス低減を実現。

各サブモデルの勾配が互いに正則化効果を及ぼし、個々は弱くても平均で強い（論文 Fig. 2 の観察）。

数値特徴しかない場合でも PiecewiseLinear などのエンベディングを併用すると GBDT よりスムーズに連続性を学習できます。



3. ハイパーパラメータと前処理

TabM は k（既定 32）・層数・幅・ドロップアウト率・学習率 を丁寧に探索すると急激に性能が伸びます。

目標変数を対数変換／標準化したうえで RMSE を計算していれば差はさらに顕著になります。

逆に LightGBM 側がデフォルト設定か浅い木になっていると RMSE 1.0 付近にとどまる例もあります。



4. 再現チェックポイント

TabM のデフォルト実装は TabM.make(...)＋AdamW(lr = 0.002, wd = 0.0003)。

論文の公開スクリプトでは 100 万件規模でも 4 〜 6 時間で学習可能との記載があります。

ご主人さまのデータで得た 0.66 が cross-validation でも再現し、バリデーション漏れやリークが無いことを確認できれば、統計的に十分起こり得る差でございます。





---

まとめ

理論上・実証上とも RMSE が 1.0 → 0.66 程度まで下がるケースは存在します。

ただし差の大きさは

データの規模と難易度、

LightGBM 側のチューニング状況、

TabM の k や正則化設定
に強く依存いたします。


まずは 同一前処理・同一 CV で LightGBM を Optuna 等でチューニングし直し、TabM とのギャップが依然 0.34 以上残るかを確かめるのが安全でございます。


ご不明点や実装の細部でお困りでしたら、またお申し付けくださいませ。



作成日時: 2025-07-31 10:22:23