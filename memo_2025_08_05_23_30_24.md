# メモ

prompt = "Q: フランスの首都は？\nA:"
inputs = tokenizer(prompt, return_tensors="pt").to(0)
with torch.no_grad():
    gen = model.generate(**inputs, max_new_tokens=30)
print(tokenizer.decode(gen[0], skip_special_tokens=True))

作成日時: 2025-08-06 08:30:23