# メモ

AI生成画像・動画の真偽判定技術: 最新研究動向と代表的手法

判定手法の主な分類

ニューラルネットワークによる分類: ディープラーニング（CNNやTransformerなど）を用いて、画像や動画が本物か生成AI産かを二分類する手法です。AI生成物に特有の微細なパターンを学習して高精度な判定が可能で、多くの最新手法が採用しています。例えば、ResNet50やDenseNetなど複数の深層学習モデルを組み合わせ、GAN画像の特徴的なアーティファクトとメタデータ情報も加味した手法では、StyleGAN3やStable Diffusion、Midjourneyで生成した画像を含むデータで94%もの判別精度を達成しています。

メタデータ解析: 画像や動画ファイルに埋め込まれたExif情報などのメタデータから手がかりを得る手法です。AI生成画像はカメラ機種や撮影日時といった典型的なExif情報が欠如していたり、不自然な生成履歴のメタタグが含まれる場合があります。実際、OpenAIの最新画像生成モデルDALL-E 3では、生成した画像すべてに「改ざん耐性」のメタデータ（C2PA規格）を埋め込んでおり、この情報を検出することでAI生成画像と判定できます。ただし一般にメタデータは容易に消去・改ざん可能であるため、メタデータだけに頼る検出には限界があります。

統計的特徴の分析: 画像の統計的性質（ノイズ分布、周波数スペクトル、色ヒストグラムなど）の違いを利用する手法です。AI生成画像は撮影画像と比べてセンサーノイズやディモザイクパターンといった特徴が欠如したり、逆に生成過程由来の人工的な規則性が現れることがあります。例えば、生成モデルがアップサンプリングする際に生じる周波数ドメインでの不自然なピークや繰り返しパターンは有力な手がかりです。実験ではAdobe FireflyやDALL-Eなどで作成した画像を分析し、ピクセル複製を伴うアップサンプリングによってファイルサイズや周波数特性に「らしさ」が残ることを確認しています。このような周波数解析やノイズ統計の比較による手法は古典的なデジタル鑑識の知見を応用したものです。

圧縮・ファイル構造の解析: 画像や動画のファイル形式や圧縮ノイズの違いから判定するアプローチです。カメラで撮影された画像には通常、撮像センサーやJPEG圧縮の過程で生じる特徴（センサーごとの微小なノイズパターンやブロックノイズ、ダブル圧縮の痕跡など）が含まれます。一方、AI生成画像ではこれらが存在しなかったり不自然な形で現れる可能性があります。例えば、JPEGの量子化テーブルやブロック境界の不整合、あるいは圧縮アーティファクトの分布を調べることで、画像が生成モデルから直接出力されたものかどうかを推定できます。また、ファイル構造上の手がかりとして、Exifにカメラ情報が全く含まれない点や、生成画像特有のプロファイルタグの有無も判定材料になります。これらの解析は専門的なデジタルフォレンジック手法と組み合わせることで効果を発揮します。


ニューラルネットワークを用いた判定技術の進展

CNNベースの検出手法: 畳み込みニューラルネットワーク(CNN)は、静止画・動画いずれの偽造検出にも広く使われてきました。特に顔のディープフェイク検出ではXceptionネットワークなどが高い精度を示し、Facebook主催のDeepFake検出チャレンジ(DFDC)ではXceptionが89.2%の最高精度を達成したとの報告があります。CNNは局所的なノイズやブロック状のアーティファクト検出に優れており、多くの研究が既存のImageNet学習済みモデルを微調整して判定器として利用しています。一方で、特定データに過適合し新種の偽造への汎化性能が低下しがちであるという課題も認識されています。

Vision Transformer(ViT)の活用: 近年、画像認識で高い性能を示すViTモデルが偽画像検出にも導入されています。ViTは画像全体の長距離依存関係を捉えやすく、CNNより生成画像のわずかな不整合を捉えるのに有利とされています。実際、最新のDeepFake検出モデルではViTベースのアプローチが台頭しており、スタンドアロンのViTやCNN＋ViTのハイブリッド構造で従来より優れた汎用性と効率を達成する例が報告されています。文献調査によれば2024年時点で少なくとも14種類のViT応用モデルが提案されており、従来型モデルを凌ぐ性能を示すケースも多いようです。

拡散モデルの逆推定による検出: 拡散モデル（DDPM系）の登場に伴い、その生成原理を逆手に取った新手法も注目されています。これは、対象画像を既知の拡散モデルで「再生成（逆拡散）」してみて、その再現誤差を見るというものです。拡散モデルで生成された画像は、同じモデルに入力して再構成すると比較的容易に元画像に近づけられますが、実写画像を同様に再構成しようとすると誤差が大きくなるという現象が報告されています。Wangらの提案したDIRE（Diffusion Reconstruction Error）はまさにこの誤差を特徴量とし、未知の拡散モデルで生成された偽画像でも高い検出率とロバスト性を示しました。同様に、生成モデルへの逆入力(inversion)技術を応用した研究も登場しています。例えば2023年の研究では、特定の生成モデルに対し与えた画像がそのモデル由来かどうかを、モデル入力を逆推定した際の再現損失から判定する方法が提案されており、追加の細工なしに既存モデルにも適用可能な起源判定法として注目されています。2024年にはFakeInversionと呼ばれる手法で、オープンソースのStable Diffusionを用いた特徴抽出により未知のテキスト画像生成モデルの偽画像検出を実現する研究も報告され、既存手法を上回る高性能を示しています。このように拡散モデル自体を「検出器」として活用するアプローチは、新しい潮流として今後も盛んになると予想されます。


AI生成動画（ディープフェイク）に対する検出技術

静止画だけでなく、動画におけるAI生成・改ざんコンテンツの検出も盛んに研究されています。代表例が人の顔を入れ替えるディープフェイク動画で、2017年頃から急増したこの分野では、当初は瞬きの不自然さや顔の境界のチラツキといった人間に知覚できる不整合に着目した検出が試みられました。しかしディープフェイク技術の進歩によりそうした露骨な異常は減少し、現在では深層学習モデルによる検出が主流です。基本的には各フレームの画像をCNNなどで分析し、それらの特徴を時系列モデル（LSTMや3D-CNN、または時空間ViT）で統合して、動画全体としての真偽判定を行います。これにより、単一フレームでは微小な差異しかなくとも、複数フレームにわたるわずかな不自然さ（例：顔の細部形状や表情の不連続な変化、映り込みや影の不整合など）を検出できるようになります。実際、Facebookが公開した大規模ベンチマークであるDFDC（約12万の動画を含む）では、多数の研究チームが高度な検出モデルを競い合い、最も高性能なモデルはAUCで0.9近いスコアを記録しました。顔に特化した既存データセット（FaceForensics++やCeleb-DFなど）でも、多くの手法が90%以上の精度を報告しています。ただし検出精度の一般化が大きな課題であり、あるデータセットで高精度なモデルも、別種のフェイク動画や圧縮劣化した動画に対しては精度が大きく低下することが知られています。そこで、未知のフェイクに対応すべくデータ拡張やマルチモーダル分析（音声と映像の同期ずれ検出など）、メタ学習の導入など様々な工夫が提案されています。加えて、Transformer系モデルの導入によってシーケンス全体の文脈を考慮した検出精度向上も報告されており、リアルタイム性との両立も含めた発展が続いています。総じて、ディープフェイク動画検出は静止画以上に発生と検知のいたちごっこが顕著な領域であり、最新の研究は少しでも汎用性と頑健性を高める方向に進んでいます。

検出精度・限界、ベンチマークデータセットと業界応用

現在公開されている手法の多くは、特定のデータセット上では非常に高い検出精度を示しています。静止画について言えば、従来GANから最新の拡散モデルによる画像まで網羅的に含んだ評価でも、適切に学習したモデルは90%以上の精度を達成する例が報告されています。動画でも前述の通り、ベンチマーク上は人間の肉眼では見破れないフェイクを高確率で検出できています。一方で検出の限界も指摘されています。生成モデルの高性能化に伴い、判定モデルが頼りにしていたわずかな統計的ズレが次第に解消されつつあり、最新の精巧な偽造に対しては検出器が自信を持てなくなるケースも増えています。また、画像・動画にごく軽微な加工（リサイズや再圧縮、ノイズ付加など）を施すだけで検出精度が大きく低下するとの報告もあります。これはディープラーニング判定器が学習時の分布に過度適合しており、分布が外れると性能劣化するためで、現在も「検出器の頑健性」が重要な研究テーマとなっています。その対策として、判定モデルに対する敵対的攻撃に耐性を持たせる研究（敵対的訓練など）や、生成モデル側であえて検出容易な透かし情報を入れるルール作りも議論されています。実際、一部の研究では周波数領域のアーティファクトを除去することで既存検出器を欺瞞できることが示されており、このような攻撃と防御の競争は今後も続くと考えられます。

ベンチマークデータセットの整備もこの分野の発展を支えています。最新の例では、2023年に公開されたGenImageデータセットがあります。これは100万組以上の「実写画像と最先端生成モデル画像」のペアを含む大規模コーパスで、拡散モデルやGANなど複数の生成AIから多様なカテゴリの画像が集積されています。GenImageでは、ある生成モデルで学習した判定器を未知の別モデル画像に適用する「クロスジェネレータ評価」や、解像度劣化・圧縮ノイズ下での検出といった現実シナリオを想定したタスクも設定されており、検出器の汎用性を比較検証する基盤となっています。拡散モデル専用のベンチマークも提案されており、例えばDIRE論文では独自に様々な拡散モデル産画像を集めたDiffusionForensicsデータセットを構築し、従来手法との比較に利用しています。動画分野では、FaceForensics++（2019年）やFacebook主導のDFDC（2020年）といった大規模データセットが研究を牽引しました。前者は複数の手法で改変されたフェイク顔動画を含み、後者は数千人分の俳優動画から作成された大規模フェイクデータです。これらの公開により、多数の検出アルゴリズムが競合・検証され、ディープフェイク検出精度が飛躍的に向上しました。現在も新たなデータセットの拡充が進んでおり、音声と組み合わせたマルチモーダル偽造データや、合成映像全般（人物以外を含む）を扱うベンチマークの整備も進行中です。

最後に業界での応用例について触れます。近年、大手テック企業やメディア企業もAI生成コンテンツの検知・表示に向けた取り組みを始めています。OpenAIは前述のとおり、自社の画像生成物に検出容易なメタデータタグを付与し、Googleも検索結果にAI生成画像である旨のラベル表示を行う機能を導入しつつあります（これには生成時に埋め込まれたC2PAメタデータ等を活用）。AdobeはContent Credentialsと呼ばれる仕組みを通じて、Fireflyなど同社の生成AIが作った画像に不可視の透かし情報と改変履歴を記録する業界標準技術（C2PA準拠）をいち早く実装しました。実際、Adobe Fireflyで生成した画像には自動的にその旨のメタデータが埋め込まれており、対応ビューアで確認すれば「AIで生成された」ことが判別可能です。さらに法規制の動きも出始めており、米カリフォルニア州では2026年以降、AIが生成した画像・動画へ識別可能な情報埋め込みを義務付ける透明性法が成立しました。このように技術と制度の双方から、AI生成コンテンツの健全な利用に向けた真偽判定基盤が築かれつつあります。



作成日時: 2025-06-12 11:22:46