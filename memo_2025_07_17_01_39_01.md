# メモ

ご主人さま、

pytorch_tabular では オプティマイザ（optimizer） と 学習率スケジューラ（lr_scheduler） をそれぞれ OptimizerConfig で指定します。
「cos」は CosineAnnealingLR スケジューラの略称と考えてよいので、以下のように設定すれば、指定したエポックで学習率を 0 まで滑らかに落とせます。

from pytorch_tabular import TabularModel
from pytorch_tabular.config import (
    DataConfig, TrainerConfig, OptimizerConfig
)
from pytorch_tabular.models import CategoryEmbeddingModelConfig  # 例

EPOCHS = 50                   # 何エポック目で LR=0 にしたいか
BASE_LR = 1e-3               # 初期学習率

opt_config = OptimizerConfig(
    optimizer="AdamW",                  # 好きなオプティマイザ
    optimizer_params={"lr": BASE_LR},   # 初期 LR
    lr_scheduler="CosineAnnealingLR",   # ← CosineAnnealing を指定
    lr_scheduler_params={
        "T_max": EPOCHS,   # ここで指定したエポック数で cosカーブが底（LR=eta_min）に到達
        "eta_min": 0.0     # 最小 LR（0 に設定）
    },
    lr_scheduler_interval="epoch"       # デフォルトが "epoch" なので省略可
)

trainer_config = TrainerConfig(
    max_epochs=EPOCHS,      # T_max と合わせるのが一般的
    gpus=None               # CPU なら None、GPU を使う場合は整数やリストで指定
)

data_config   = DataConfig(  # 例：最低限の必須項目
    target=["target"],
    continuous_cols=["num1", "num2"],
    categorical_cols=["cat1"]
)

model_config  = CategoryEmbeddingModelConfig()  # 好きなモデル

model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=opt_config,
    trainer_config=trainer_config,
)
model.fit(train=df_train, validation=df_val)

パラメータの意味

引数	役割

T_max	コサイン曲線が 1 周する長さ。ここで指定したエポック数後に eta_min へ到達します。
eta_min	最小学習率。0 を指定すればちょうどそのエポックで LR が 0 になります。
lr_scheduler_interval	"epoch" なら 1 エポックごとに scheduler.step() が呼ばれます。


> OptimizerConfig では optimizer・lr_scheduler・lr_scheduler_params などを文字列と辞書で与えるだけで、内部で PyTorch の torch.optim／torch.optim.lr_scheduler をラップしてくれます。
CosineAnnealingLR は T_max と eta_min を取るスケジューラで、eta_min を 0 にすれば最終的に学習率 0 になります。



追加のヒント

途中で学習を止める場合
max_epochs を T_max より小さくすると、cosカーブが途中で切れるので LR=0 にはなりません。必ず同じか大きめに設定してください。

ウォームアップを挿みたい場合
SequentialLR と ConstantLR／CosineAnnealingLR を組み合わせて、最初だけ一定 LR → その後 cos 減衰、という設定も可能です（PyTorch の torch.optim.lr_scheduler.SequentialLR を lr_scheduler に渡す）。


これでご指定の「cos スケジューラで指定エポック到達時に学習率 0」という要件を満たせます。お試しくださいませ。



作成日時: 2025-07-17 10:39:00