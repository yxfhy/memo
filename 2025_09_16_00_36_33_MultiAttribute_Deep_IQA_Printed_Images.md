# 印刷画像向け多属性ディープ画質評価

Multi-Attribute Deep Learning IQA for Printed Images

Introduction

Traditional image quality assessment (IQA) models – especially no-reference or blind IQA – typically output a single scalar value (e.g. a MOS prediction) to represent overall quality. While useful for ranking images, a lone quality score offers little insight into why an image is of poor quality. In domains like printing and photography, quality is inherently multi-dimensional, encompassing factors such as sharpness, noise/graininess, contrast, color fidelity, and more. Assessing these attributes separately can provide interpretable feedback (e.g. “the image is sharp but very noisy”) that is valuable for diagnosing issues, improving designs, or tuning processes. Recent research in deep learning-based IQA has begun to address this need by developing multi-attribute IQA models that predict multiple quality dimensions or factors instead of a single MOS. This report surveys such approaches – with a focus on printed image quality evaluation – and highlights representative techniques (like MDIQA) that output attribute-wise quality predictions useful for design feedback and process optimization. We also broaden the scope to related interpretable no-reference IQA methods (multi-task models, quality factor analysis, etc.) given the limited number of print-specific multi-attribute IQA systems.

Multi-Dimensional Image Quality Assessment (MDIQA)

One prominent example is MDIQA (Multi-Dimensional IQA), a deep learning framework explicitly designed to rate images across multiple perceptual dimensions. MDIQA was inspired by the observation that humans naturally evaluate image quality along different axes (technical factors like clarity or noise, and aesthetic factors like composition or lighting). Instead of a single branch predicting MOS, MDIQA uses a multi-branch neural network with separate pathways for different groups of quality attributes. In the implementation by Wu et al. (2023), the model is split into a technical quality branch and an aesthetic quality branch, each with its own backbone network and multiple lightweight heads for specific dimensions. For example, the technical branch might output metrics for sharpness, noisiness, brightness, color accuracy, and contrast, while the aesthetic branch outputs ratings for composition, lighting, content, and color style. Each head produces a score for that particular attribute. An additional “weighting” sub-network then predicts how important each dimension is for the given image, allowing the model to combine the attribute scores into an overall quality score in an image-adaptive way. This adaptive weighting yields a final MOS prediction but (importantly) retains the intermediate attribute-wise scores for interpretability.

MDIQA has shown that multi-dimensional modeling can achieve state-of-the-art accuracy on standard IQA benchmarks while providing richer feedback. The per-attribute scores make the result more explainable – e.g. one can see if an image’s low quality is due to noise vs. blur vs. poor contrast – and thus more actionable. Moreover, the authors demonstrated an innovative use-case for design optimization: treating MDIQA as a learned loss function for image enhancement. By weighting the dimension-specific losses, one can bias a deep image restoration model to prioritize certain quality aspects (e.g. increase sharpness weight to reduce blur at the expense of noise). This highlights a key benefit of multi-attribute IQA in process tuning: by quantifying specific quality aspects, one can directly optimize or trade off those aspects according to requirements. In summary, MDIQA’s architecture – with dual CNN backbones and multiple prediction heads – exemplifies how deep models can produce interpretable, attribute-level IQA outputs rather than a black-box MOS. Table 1 (next section) will summarize MDIQA alongside other multi-attribute IQA methods.

Multi-Attribute and Multi-Task IQA Approaches

Beyond MDIQA, several other works have explored multi-attribute IQA, often via multi-task learning. A foundational contribution in this space was the SPAQ dataset (CVPR 2020), which provided human ratings on multiple quality attributes for over 11k smartphone photos. In SPAQ, each image has an opinion score and five attribute scores – specifically brightness, colorfulness, contrast, noisiness, and sharpness. Fang et al. not only released this rich dataset, but also trained baseline deep models to predict these attributes alongside overall quality. This established a multi-task paradigm: a CNN or CNN+FC model learns to output a vector of quality factors (the five attributes) and sometimes an overall score as an additional output. The multi-task learning was found to improve generalization, since the network learned a more structured representation of image defects (e.g. understanding noise and blur separately). SPAQ’s influence can be seen in later works that use similar attribute definitions. For instance, a very recent 2025 study by Luo et al. introduces MaC-BIQA (Multi-attribute Continual Learning BIQA). They target the same kinds of attributes (brightness, sharpness, contrast, colorfulness) but focus on continual learning across them, addressing the challenge that sequentially training on different quality attributes can lead to catastrophic forgetting. By using techniques like gated attention per attribute and knowledge-distillation between tasks, their model can learn multiple attribute predictions without forgetting earlier ones. This is important for scalability – as more quality dimensions are added, the model remains robust – and it reinforces the idea that multi-attribute IQA benefits from specialized learning strategies (in this case, a continual multi-task framework).

Another notable approach is TuningIQA (Xiao et al., 2023), developed for automatic camera tuning in livestreaming scenarios. TuningIQA is essentially a fine-grained BIQA model that integrates multi-attribute regression. Their new FGLive-10K dataset contains images captured with varied camera settings, each annotated with both an overall quality score and four specific attributes: face quality, sharpness, exposure (brightness/contrast), and noise. The emphasis on “face quality” is domain-specific (live videos often center on people’s faces), but the inclusion of sharpness, exposure, and noise is very much aligned with traditional IQA factors. TuningIQA’s architecture uses a CNN with a human-region attention module (to focus on faces) and a graph neural network to model relationships between camera parameters. Crucially, it employs a unified multi-task loss: part of the loss trains the network to regress the four attribute scores, while another part trains it to predict pairwise quality rankings (for subtle differences). This combination yielded a system that outperforms conventional single-score BIQA methods in distinguishing fine quality differences. In practice, TuningIQA can tell a camera why one setting is better than another – e.g. “Setting A produced sharper and brighter images than Setting B, at the cost of slightly higher noise” – which is invaluable for camera design optimization.

The table below summarizes key multi-attribute IQA methods and their characteristics:

Method (Year)	Output Quality Attributes	Architecture	Application Domain	Source

MDIQA (Wu et al., 2023)	5 technical (e.g. Sharpness, Noise, Brightness, Contrast, Colorfulness) + 4 aesthetic attributes, plus overall score.	Dual-backbone CNN + multi-head (per attribute) + adaptive weighting fusion (MLP). Uses CLIP features for semantic context.	General natural images (authentic distortions); also used as tunable loss for image restoration.	Wu et al., arXiv 2023
SPAQ Multi-task (Fang et al., 2020)	5 attributes (Brightness, Colorfulness, Contrast, Noise, Sharpness) and MOS.	Baseline CNN (ResNet-18) with multi-output regression; also tried a multi-task ranking network.	Smartphone photography quality (photos from 66 phone models).	Fang et al., CVPR 2020
MaC-BIQA (Luo et al., 2025)	Similar set of technical attributes (brightness, sharpness, contrast, color, etc.) – learned sequentially as separate “tasks”.	CNN backbone with gated attention per attribute; uses Knowledge Graph Embedding to retain inter-attribute relations. Employs Learning without Forgetting for continual multi-task learning.	General BIQA (tested on KonIQ-10k, SPAQ). Aim is robust multi-attribute learning across diverse content.	Luo et al., ISCAS 2025 (In Press)
TuningIQA (Xiao et al., 2023)	4 attributes (Face region quality, Sharpness, Exposure, Noise) + overall quality. Also provides pairwise quality rankings.	Swin-Transformer-based CNN with human-region attention; Graph Neural Network fuses 7 camera settings. Multi-task loss: attribute regression + ranking loss.	Live video frame quality (for camera parameter tuning – exposure, focus, etc.). Fine-grained assessment of subtle quality changes.	Xiao et al., arXiv 2023
DOVER (Wu et al., 2023) – Video QA	Technical vs Aesthetic quality scores (2 outputs), further broken down in analysis.	Two-stream model disentangling technical vs aesthetic factors in video; uses regression for each and combines.	User-generated video quality (UGC-VQA) – first approach with dual outputs for clear analysis.	Wu et al., ICCV 2023 (DOVER)


Table 1: Representative deep IQA methods producing multiple quality outputs. Each approach is designed to predict interpretable quality dimensions rather than a single MOS. These range from image-wide attributes (e.g. noise, sharpness) to specialized factors (face quality in livestreaming frames, technical vs. aesthetic video quality). All use multi-branch or multi-task deep learning networks to achieve attribute-wise evaluation.

Printed Image Quality Evaluation and Attributes

Printed image quality brings its own set of unique artifacts and attributes. Common print defects include mottle (low-frequency unevenness in solid areas), banding (periodic lines from mechanical or imaging errors), graininess (high-frequency micro-unevenness or noise in uniform tones), misregistration (color plane misalignment causing fringes), blur (due to print head or motion issues, affecting sharpness), and uniformity issues (streaks, blotches, etc.). Historically, print quality engineers have measured these aspects with separate metrics or visual inspections – for example, ISO standards exist to quantify graininess and mottle (macro- and micro-uniformity) and line quality in prints. An early comprehensive tool (circa 1999) could compute the strengths of various print-quality (PQ) defects from scanned pages using ISO-recommended procedures. That tool and related “PQ troubleshooting” pages provided separate scores for defects like banding or mottling, aiding press operators in diagnosing issues.

Despite this need for multi-factor evaluation, deep learning approaches in the printing domain have been limited and often focus on one artifact at a time. For instance, Chen et al. (2020) tackled printed mottle defect grading using a CNN classifier. They scanned printed samples and trained a ResNet-based model to classify the severity of mottle into four grades (A to D, from none to worst). While successful (achieving ~80%+ accuracy) in automating what used to be a subjective visual check, this model only addresses one attribute (mottle uniformity). In principle, one could train separate networks for other defects (e.g. a banding detector, a graininess regressor), but that siloed approach misses the opportunity to jointly evaluate trade-offs (e.g. an image might have slight banding but excellent sharpness and color). A multi-attribute deep IQA for prints would ideally output several print-specific quality indices at once – e.g. “Graininess: 0.2 (low), Banding: 0.8 (high), Sharpness: 0.9 (good), Color Accuracy: 0.95”, etc. This could directly translate into actionable feedback (for example, high banding score could point to a drum encoder issue, high graininess could suggest needing better paper or tuning dither patterns).

Researchers are beginning to move in this direction. A recent study by Zhong et al. (2024) proposed a “Multidimensional Digital Printing Image Quality” metric. Rather than a deep CNN predicting attributes, they engineered a pipeline combining multiple feature extractors and then used a shallow neural network as a regressor. Figure 1 below illustrates their system: a scanned print and its original digital image are processed in parallel to extract a variety of spatial and frequency domain features, which are compared to produce several component similarity measures (gradient/sharpness similarity, texture similarity, color fidelity, spatial-frequency similarity, structural similarity). These are essentially multi-dimensional print quality indicators. Finally, a small MLP merges those factors into an overall print quality score. Notably, this method achieved higher correlation with human print quality judgments than previous single-metric approaches. While the model outputs a fused score in the end, the intermediate measures (sharpness/texture/color/etc. similarities) provide a level of interpretability akin to multi-attribute outputs. In effect, it breaks down print quality into multiple factors before recombining, which aligns with the goal of attribute-wise evaluation.

 Figure 1: Workflow of a multidimensional print-quality metric. A scanned printed image and its original (pre-press) image are compared across various features – gradients (sharpness), textures, colors, spatial frequencies, structural details – yielding multiple similarity scores. An MLP then fuses these into an overall quality score. Such frameworks highlight the importance of evaluating multiple quality attributes (sharpness, graininess/noise, color accuracy, etc.) for printed images rather than a single monolithic metric.

In industry, companies like Canon and HP are keenly interested in such attribute-wise evaluations. Although specific proprietary methods are not publicly detailed, Canon’s AI imaging initiatives hint at “three areas” of image quality improvements – often noise reduction, resolution (sharpness), and color rendering – reflecting the same multi-factor perspective. A multi-attribute IQA model could, for example, guide a printer’s calibration: if a print’s “graininess score” is high (too grainy) but “sharpness score” is low, the printer might increase smoothing or adjust screening algorithms to reduce noise at the cost of some resolution, until an optimal balance is achieved. This kind of closed-loop quality optimization is feasible only when quality is described in interpretable dimensions.

In summary, the printing domain stands to benefit greatly from deep IQA systems that output multiple quality attributes. While current deep models dedicated to print IQA are still nascent (often tackling one defect at a time), the trend in general IQA toward multi-dimensional assessment can be applied. Combining attributes like graininess, banding, sharpness, and color accuracy into one model (much like MDIQA does for generic images) is an open research opportunity, with potentially significant payoff in print process control and design feedback loops.

Interpretable and Hybrid Approaches

When true multi-attribute prediction is difficult (due to limited data for certain quality factors, etc.), other strategies can provide partial interpretability in no-reference IQA:

Distortion Identification as Side-Output: Several classic BIQA methods first classified the distortion type before predicting quality. For example, the BIQI index (Moorthy & Bovik 2010) would internally determine if an image was blurred, JPEG-compressed, noisy, etc., and then apply a specialized quality predictor. Deep learning analogs of this idea exist – e.g. DB-CNN (Zhang et al., 2018) had a dual CNN where one branch was trained to classify distortion type/level and the other extracted features, combining them to predict quality. The distortion type and level in effect serve as interpretable indicators (telling us the primary degradation). Similarly, multi-task training can be used to predict a category of defect alongside the quality score. In a print context, one could imagine a model that outputs, say, “defect present: banding” (yes/no) as well as an overall score – giving insight into why the print scored poorly.

Quality Maps / Spatial Indicators: Instead of multiple named attributes, some deep IQA methods produce spatial quality maps highlighting which regions are low quality. An example is the PaQ-2-PiQ model (CVPR 2019) which predicts a coarse quality heatmap over the image and then pools it to an overall score. This is partially interpretable since the heatmap shows if, for instance, background blur or foreground noise is the issue, even if it doesn’t explicitly name the attribute. For printed images, a spatial map could show where on the page defects occur (e.g. emphasizing streaks or mottled regions).

Explainable AI and Descriptions: Emerging work attempts to leverage large vision-language models to make IQA more explainable. One recent paper, IQAGPT (Chen et al., 2024), integrates a vision-language model that outputs a textual report on image quality. In a medical imaging scenario, IQAGPT was prompted to describe image quality issues and then give a score. The generated text might say, for example, “The image is slightly blurred and has moderate noise, but overall diagnostic content is preserved”, which essentially provides attribute-level commentary. While still experimental, this approach points to a future where an IQA system might explain its rating by mentioning multiple factors (much like a human examiner would).

Aesthetic & Technical Factor Separation: Some works focus on interpretability by separating technical quality vs. aesthetic quality. We saw this in MDIQA and in the video model DOVER. Another image example is the ATAQA framework (Jin et al., 2023) which used a two-stage model: first generate descriptions of aesthetic attributes, and then predict quality. Although aimed at aesthetics, it provides an interpretable breakdown (e.g. “composition is good, exposure is poor”). This concept could extend to printing where “technical quality” (physical print fidelity) is separated from “content quality” (e.g. is the design itself high quality), providing a structured evaluation.


In closing, truly interpretable no-reference IQA is still an active research frontier. Multi-attribute deep models like MDIQA, MaC-BIQA, and TuningIQA represent significant progress by producing explicit quality factors. For printing applications, these approaches promise more insightful quality control – pinpointing whether a print issue is due to sharpness loss, excessive grain, color errors, or other factors. Such granular feedback is invaluable for designers and engineers: a designer can get objective feedback on, say, “blurred text” vs. “grainy photos” in a printed proof, and an engineer can adjust the print process to target the specific shortcoming. As datasets grow and techniques like multi-task learning, continual learning, and vision-language models mature, we anticipate more robust and specialized multi-attribute IQA systems tailored to the print industry. These will enable a shift from monolithic quality scores to comprehensive quality profiles, ultimately leading to better print quality optimization and more transparent image quality assessment across the board.

References (Key Sources)

Wu et al. (2023), “MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration”

Fang et al. (2020), “Perceptual Quality Assessment of Smartphone Photography (SPAQ Database)”

Luo et al. (2025), “Multi-attribute Continual Learning for Blind IQA” (IEEE ISCAS)

Xiao et al. (2023), “TuningIQA: Fine-Grained BIQA for Livestreaming Camera Tuning”

Chen & Allebach (2020), “Deep Learning for Printed Mottle Defect Grading” (IS&T EI)

Zhong et al. (2024), “Multidimensional Digital Printing IQ Evaluation via MLP” (Applied Sciences)

Chen et al. (2024), “IQAGPT: IQA with Vision-Language and ChatGPT (CT Medical Images)”

Wu et al. (2023), “DOVER: Disentangled Objective Video Quality Evaluator (Technical vs Aesthetic)”, ICCV 2023.




作成日時: 2025-09-16 09:36:26