# メモ

はい、ご主人さま。あります。代表的な「AI生成画像（AIGC/AGI）専用」IQAデータセットは次のとおりです。

AGIQA-1K（2023）— 最初期のAGI用主観評価DB。1,080枚のAGIに対し、技術的問題・AI特有のアーティファクト・不自然さ・不一致・審美性といった軸でラベル（MOS）を付与。公開あり。 

AGIQA-3K（2023）— 2,982枚。**知覚品質（perception）とテキスト一致（text-to-image alignment）**の2軸で主観評価を収集。複数生成モデルを横断し、既存IQAとの整合も検証。 

AIGCIQA-2023（2023）— 約2千枚、Quality / Authenticity / Correspondenceの3視点で主観評価（“本物らしさ”やテキスト一致を含む）。 

AIGIQA-20K（CVPRW 2024）— 2万枚・42万評価の大規模DB。15種のT2Iモデルを網羅し、CFG・反復回数・解像度などハイパラも動的に振って知覚品質＋アラインメントを細粒度に収集。ベンチマーク付き。 

AGHI-QA（2025）— “人間画像”に特化。4,000枚／10モデル。知覚品質・テキスト一致に加え、可視部位と歪んだ部位のラベルも付与（手指や肢の破綻などの検出を想定）。 



---

なぜ「生成画像」だけ別扱いなのか

1. 品質の決定要因が従来のNSI（自然画像）と本質的に違うからです。従来IQAが想定する“撮像由来の歪み（ぼけ・雑音・圧縮劣化など）”ではなく、生成固有の不自然さ・解剖学的破綻・テクスチャの異常反復・文字生成の崩れなどが支配的になります。従来指標はそのままでは適用が難しい、とAIGIQA-20Kは明記しています。 


2. 評価次元が多次元化しているからです。生成物は「見た目の良し悪し（perception）」だけでなく、プロンプトとの一致（alignment）や本物らしさ／真正性（authenticity）も重要視され、データセット側でこれらを別軸で主観収集します（AGIQA-3KはPerception＋Alignmentの2軸、AIGCIQA-2023はQuality＋Authenticity＋Correspondenceの3軸）。 


3. モデル／ハイパラ／プロンプト依存性が極端に大きいからです。同じプロンプトでもモデルやCFG、反復回数、解像度で品質分布が大きく揺れます。そのため生成モデル横断＋ハイパラを振った設計の専用DBが必要になります（AIGIQA-20K）。 


4. **参照画像が存在しない前提（NR-IQA前提）**が基本だからです。生成物は“劣化前の正解画像”を持たないため、主観MOSに強く依存した設計と、アラインメント指標の併用が不可欠になります（AGIQA-3K／AIGIQA-20K）。 


5. 領域特化の品質問題があるからです。とくに“人間”は解剖学的整合性が厳しく、部位ごとの破綻を拾える専用注釈（AGHI-QAの部位ラベル等）が必要になります。 



以上より、生成画像は歪みの種類・評価次元・データ分布が従来NSIと大きく異なるため、専用のIQAデータセットが独立して整備されています。



作成日時: 2025-09-24 16:34:44